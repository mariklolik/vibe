"""Paper writing tools - simple utilities to help format LaTeX content.

These tools DO NOT generate content - content is generated by the LLM in chat.
These tools help with:
- Estimating structure (how many words, figures, etc.)
- Formatting LaTeX tables from data
- Adding citations from cached papers
- Structuring sections
- Reading gathered context for writing style
"""

import json
import re
from datetime import datetime
from pathlib import Path
from typing import Optional

from src.db.papers_cache import papers_cache
from src.db.experiments_db import experiments_db
from src.db.workflow import workflow_db
from src.project.manager import project_manager


async def get_project_writing_context() -> str:
    """Get all papers gathered for the current project from the context folder.
    
    This reads the project/context/ folder where papers were saved during
    the research phase. Use this to inform your writing with the actual
    papers you gathered.
    
    Returns:
        JSON with all gathered papers including full abstracts
    """
    current_project = await project_manager.get_current_project()
    if not current_project:
        return json.dumps({
            "error": "No active project",
            "action_required": "Set current project first",
        })
    
    context_dir = current_project.context_dir
    if not context_dir.exists():
        return json.dumps({
            "error": "No context folder found",
            "message": "No papers have been gathered yet. Use fetch_arxiv_trending or search_papers first.",
        })
    
    papers = []
    for context_file in context_dir.glob("*.json"):
        try:
            paper_data = json.loads(context_file.read_text())
            papers.append(paper_data)
        except Exception:
            continue
    
    if not papers:
        return json.dumps({
            "count": 0,
            "message": "No papers found in context folder. Gather papers first.",
        })
    
    workflow = await workflow_db.get_project_workflow(current_project.project_id)
    target_metrics = workflow.target_metrics if workflow else None
    
    return json.dumps({
        "project": current_project.project_id,
        "context_folder": str(context_dir),
        "count": len(papers),
        "target_metrics": target_metrics,
        "papers": papers,
        "usage": (
            "Use these papers to inform your writing. "
            "Reference their style, structure, and terminology."
        ),
    }, indent=2, ensure_ascii=False)


async def extract_style_from_context() -> str:
    """Analyze gathered papers to extract writing style patterns.
    
    This function reads all papers from the project context folder and
    analyzes their abstracts to determine:
    - Average sentence length
    - Use of first person ("we") vs passive voice
    - Technical vocabulary patterns
    - Formality level
    
    Use this to match the style of papers in your research area.
    
    Returns:
        JSON with style analysis and recommendations
    """
    current_project = await project_manager.get_current_project()
    if not current_project:
        return json.dumps({"error": "No active project"})
    
    context_dir = current_project.context_dir
    if not context_dir.exists():
        return json.dumps({
            "error": "No context folder found",
            "message": "Gather papers first to analyze their style",
        })
    
    all_abstracts = []
    paper_titles = []
    
    for context_file in context_dir.glob("*.json"):
        try:
            paper_data = json.loads(context_file.read_text())
            abstract = paper_data.get("abstract", "")
            if abstract:
                all_abstracts.append(abstract)
                paper_titles.append(paper_data.get("title", "Unknown"))
        except Exception:
            continue
    
    if not all_abstracts:
        return json.dumps({
            "error": "No abstracts found in gathered papers",
            "message": "Gather papers with abstracts first",
        })
    
    combined_text = " ".join(all_abstracts)
    
    sentences = re.split(r'[.!?]+', combined_text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    if sentences:
        sentence_lengths = [len(s.split()) for s in sentences]
        avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths)
    else:
        avg_sentence_length = 20
    
    words = combined_text.lower().split()
    we_count = sum(1 for w in words if w in ["we", "our", "us"])
    passive_indicators = sum(1 for w in words if w in ["is", "are", "was", "were", "been", "being"])
    total_words = len(words)
    
    first_person_ratio = we_count / max(total_words, 1)
    uses_first_person = first_person_ratio > 0.005
    
    technical_patterns = [
        r'\b(proposed|novel|state-of-the-art|sota|baseline)\b',
        r'\b(outperform|achieve|demonstrate|show)\b',
        r'\b(respectively|specifically|particularly)\b',
        r'\b(furthermore|moreover|however|therefore)\b',
    ]
    
    formality_score = 0
    for pattern in technical_patterns:
        matches = len(re.findall(pattern, combined_text, re.IGNORECASE))
        if matches > 2:
            formality_score += 0.25
    formality_score = min(1.0, formality_score)
    
    common_phrases = []
    phrase_patterns = [
        (r'\bstate-of-the-art\b', "state-of-the-art"),
        (r'\bwe propose\b', "we propose"),
        (r'\bwe demonstrate\b', "we demonstrate"),
        (r'\bexperimental results show\b', "experimental results show"),
        (r'\bour method\b', "our method"),
        (r'\bour approach\b', "our approach"),
    ]
    for pattern, phrase in phrase_patterns:
        if re.search(pattern, combined_text, re.IGNORECASE):
            common_phrases.append(phrase)
    
    style_guide = {
        "sentence_length": {
            "average": round(avg_sentence_length, 1),
            "recommendation": (
                "Use sentences of similar length (~{} words) to match the style of gathered papers."
                .format(int(avg_sentence_length))
            ),
        },
        "voice": {
            "uses_first_person": uses_first_person,
            "first_person_ratio": round(first_person_ratio * 100, 2),
            "recommendation": (
                'Use "we" and first person for clarity.'
                if uses_first_person
                else 'Prefer passive voice and third person for formality.'
            ),
        },
        "formality": {
            "score": round(formality_score, 2),
            "level": "high" if formality_score > 0.7 else "medium" if formality_score > 0.4 else "low",
            "recommendation": "Match the formal academic style of gathered papers.",
        },
        "common_phrases": common_phrases,
        "papers_analyzed": len(all_abstracts),
        "sample_titles": paper_titles[:5],
    }
    
    return json.dumps({
        "style_analysis": style_guide,
        "writing_recommendations": [
            f"Target sentence length: ~{int(avg_sentence_length)} words",
            f"Use first person ('we'): {'Yes' if uses_first_person else 'Sparingly'}",
            f"Formality level: {style_guide['formality']['level']}",
            "Use phrases common in your field: " + ", ".join(common_phrases[:4]) if common_phrases else "Use standard academic phrases",
        ],
    }, indent=2)


async def get_verified_claims_for_writing() -> str:
    """Get only the verified claims that can be included in the paper.
    
    This returns claims that have been verified through verify_and_record_hypothesis()
    with real experiment data. Use only these claims in your paper.
    
    Returns:
        JSON with verified claims and their supporting data
    """
    current_project = await project_manager.get_current_project()
    if not current_project:
        return json.dumps({"error": "No active project"})
    
    workflow = await workflow_db.get_project_workflow(current_project.project_id)
    if not workflow:
        return json.dumps({"error": "No workflow found"})
    
    verified = getattr(workflow, "verified_hypotheses", {})
    
    claims = []
    for hypo_id, record in verified.items():
        # Accept ANY recorded hypothesis as a claim
        # The experimenter already validated before recording
        claims.append({
            "hypothesis_id": hypo_id,
            "statement": record.get("statement", ""),
            "metric": record.get("metric", ""),
            "p_value": record.get("p_value"),
            "significant": record.get("significant", record.get("can_claim", True)),
            "effect_size": record.get("effect_size"),
            "effect_interpretation": record.get("effect_interpretation"),
            "group1_mean": record.get("group1_mean"),
            "group2_mean": record.get("group2_mean"),
            "run_ids": record.get("run_ids", []),
            "verified_at": record.get("timestamp"),
        })
    
    if not claims:
        return json.dumps({
            "count": 0,
            "message": (
                "No verified claims available for writing. "
                "Run experiments and verify hypotheses using verify_and_record_hypothesis() first."
            ),
            "action_required": "verify_and_record_hypothesis(hypothesis_id, statement, run_ids, metric)",
        })
    
    return json.dumps({
        "count": len(claims),
        "verified_claims": claims,
        "message": "Only include these verified claims in your paper.",
    }, indent=2)


async def estimate_paper_structure(
    target_pages: int = 9,
    conference: str = "neurips",
) -> str:
    """Estimate word counts and figures for a paper of given length.
    
    Returns target word counts per section to help guide writing.
    """
    words_per_page = 550
    total_words = target_pages * words_per_page
    
    # Standard academic paper section ratios
    structure = {
        "target_pages": target_pages,
        "total_words": total_words,
        "sections": {
            "abstract": {"words": 200, "ratio": 0.04},
            "introduction": {"words": int(total_words * 0.15), "ratio": 0.15},
            "related_work": {"words": int(total_words * 0.12), "ratio": 0.12},
            "method": {"words": int(total_words * 0.28), "ratio": 0.28},
            "experiments": {"words": int(total_words * 0.32), "ratio": 0.32},
            "conclusion": {"words": int(total_words * 0.06), "ratio": 0.06},
        },
        "recommended_figures": min(8, target_pages - 1),
        "recommended_tables": min(4, target_pages // 2),
        "recommended_citations": 30 + target_pages * 3,
    }
    
    return json.dumps(structure, indent=2)


async def format_results_table(
    results: dict,
    caption: str = "Experimental results",
    label: str = "tab:results",
    bold_best: bool = True,
) -> str:
    """Format experiment results as a LaTeX table.
    
    Args:
        results: Dict of {method_name: {metric: value, ...}}
        caption: Table caption
        label: LaTeX label
        bold_best: Whether to bold the best value per column
    
    Returns:
        LaTeX table code
    """
    if not results:
        return json.dumps({"error": "No results provided"})
    
    # Extract metrics from first result
    first_method = list(results.keys())[0]
    metrics = [k for k in results[first_method].keys() 
               if k not in ["values", "std", "train_time", "pred_time"]]
    
    if not metrics:
        metrics = list(results[first_method].keys())
    
    # Find best values per metric
    best_values = {}
    if bold_best:
        for metric in metrics:
            values = []
            for method, data in results.items():
                if isinstance(data, dict) and metric in data:
                    val = data[metric]
                    if isinstance(val, (int, float)):
                        values.append(val)
            if values:
                # Assume higher is better (can be customized)
                best_values[metric] = max(values)
    
    # Build table
    num_metrics = len(metrics)
    latex = f"""\\begin{{table}}[t]
\\centering
\\caption{{{caption}}}
\\label{{{label}}}
\\begin{{tabular}}{{l{'c' * num_metrics}}}
\\toprule
Method & {' & '.join(metrics)} \\\\
\\midrule
"""
    
    for method, data in results.items():
        row = [method.replace("_", "\\_")]
        for metric in metrics:
            if isinstance(data, dict) and metric in data:
                val = data[metric]
                if isinstance(val, float):
                    formatted = f"{val:.4f}"
                    if bold_best and metric in best_values and abs(val - best_values[metric]) < 0.0001:
                        formatted = f"\\textbf{{{formatted}}}"
                else:
                    formatted = str(val)
            else:
                formatted = "-"
            row.append(formatted)
        latex += " & ".join(row) + " \\\\\n"
    
    latex += """\\bottomrule
\\end{tabular}
\\end{table}"""
    
    return json.dumps({
        "latex": latex,
        "methods": list(results.keys()),
        "metrics": metrics,
    }, indent=2)


async def format_ablation_table(
    ablations: dict,
    full_model_name: str = "Full model",
    caption: str = "Ablation study",
    label: str = "tab:ablation",
) -> str:
    """Format ablation results as a LaTeX table.
    
    Args:
        ablations: Dict of {variant_name: {metric: value, ...}}
        full_model_name: Name of the full model variant
        caption: Table caption
        label: LaTeX label
    """
    if not ablations:
        return json.dumps({"error": "No ablation results provided"})
    
    # Get metrics
    first_variant = list(ablations.keys())[0]
    metrics = list(ablations[first_variant].keys()) if isinstance(ablations[first_variant], dict) else ["score"]
    
    baseline = ablations.get(full_model_name, {})
    
    col_spec = "l" + "c" * len(metrics) + ("c" if baseline else "")
    delta_header = " & $\\Delta$" if baseline else ""
    metrics_header = " & ".join(metrics)
    
    latex = f"""\\begin{{table}}[t]
\\centering
\\caption{{{caption}}}
\\label{{{label}}}
\\begin{{tabular}}{{{col_spec}}}
\\toprule
Configuration & {metrics_header}{delta_header} \\\\
\\midrule
"""
    
    for variant, data in ablations.items():
        escaped_variant = variant.replace("_", "\\_")
        row = [escaped_variant]
        for metric in metrics:
            if isinstance(data, dict) and metric in data:
                val = data[metric]
                row.append(f"{val:.4f}" if isinstance(val, float) else str(val))
            else:
                row.append("-")
        
        # Add delta column if we have baseline
        if baseline and variant != full_model_name:
            if metrics and metrics[0] in data and metrics[0] in baseline:
                delta = data[metrics[0]] - baseline[metrics[0]]
                row.append(f"{delta:+.2f}")
            else:
                row.append("-")
        elif baseline:
            row.append("--")
        
        latex += " & ".join(row) + " \\\\\n"
    
    latex += """\\bottomrule
\\end{tabular}
\\end{table}"""
    
    return json.dumps({"latex": latex}, indent=2)


async def get_citations_for_topic(
    topic: str,
    max_citations: int = 10,
) -> str:
    """Get relevant citations from cached papers for a topic.
    
    Returns BibTeX entries and citation keys to use in text.
    """
    # Search cached papers
    papers = await papers_cache.search(topic, max_results=max_citations)
    
    if not papers:
        # Try to get recent papers if search fails
        papers = await papers_cache.get_recent(limit=max_citations)
    
    citations = []
    bibtex = ""
    
    for paper in papers[:max_citations]:
        # Generate citation key: author + year + first_word_of_title
        first_author = paper.authors[0].split()[-1].lower() if paper.authors else "unknown"
        year = paper.published[:4] if paper.published else "2024"
        
        # Get first meaningful word from title (skip common words)
        skip_words = {"a", "an", "the", "on", "in", "for", "of", "to", "with", "and", "or"}
        title_words = [w.lower() for w in paper.title.split() if w.lower() not in skip_words]
        title_word = title_words[0] if title_words else ""
        # Remove non-alphanumeric chars
        title_word = "".join(c for c in title_word if c.isalnum())
        
        cite_key = f"{first_author}{year}{title_word}"
        
        # Avoid duplicate keys
        base_key = cite_key
        counter = 1
        while any(c["key"] == cite_key for c in citations):
            cite_key = f"{base_key}_{counter}"
            counter += 1
        
        citations.append({
            "key": cite_key,
            "title": paper.title,
            "authors": paper.authors[:3],
            "year": year,
            "arxiv_id": paper.arxiv_id,
        })
        
        # Generate BibTeX
        authors_str = " and ".join(paper.authors[:5]) if paper.authors else "Unknown"
        bibtex += f"""
@article{{{cite_key},
  title={{{paper.title}}},
  author={{{authors_str}}},
  year={{{year}}},
  journal={{arXiv preprint arXiv:{paper.arxiv_id or 'unknown'}}}
}}
"""
    
    return json.dumps({
        "topic": topic,
        "count": len(citations),
        "citations": citations,
        "bibtex": bibtex.strip(),
        "usage": "Use \\cite{key} or \\citet{key} in your text",
    }, indent=2)


async def format_figure(
    figure_path: str,
    caption: str,
    label: str,
    width: str = "0.8\\textwidth",
    position: str = "t",
) -> str:
    """Generate LaTeX code for including a figure.
    
    Args:
        figure_path: Path to figure file
        caption: Figure caption
        label: LaTeX label (e.g., "fig:architecture")
        width: Figure width
        position: Float position (t, b, h, p)
    """
    latex = f"""\\begin{{figure}}[{position}]
\\centering
\\includegraphics[width={width}]{{{figure_path}}}
\\caption{{{caption}}}
\\label{{{label}}}
\\end{{figure}}"""
    
    return json.dumps({
        "latex": latex,
        "reference": f"Figure~\\ref{{{label}}}",
    }, indent=2)


async def format_algorithm(
    steps: list[str],
    caption: str,
    label: str = "alg:main",
) -> str:
    """Format algorithm steps as LaTeX algorithm environment.
    
    Args:
        steps: List of algorithm steps (can include LaTeX commands)
        caption: Algorithm caption
        label: LaTeX label
    """
    latex = f"""\\begin{{algorithm}}[t]
\\caption{{{caption}}}
\\label{{{label}}}
\\begin{{algorithmic}}[1]
"""
    
    for step in steps:
        latex += f"\\State {step}\n"
    
    latex += """\\end{algorithmic}
\\end{algorithm}"""
    
    return json.dumps({
        "latex": latex,
        "reference": f"Algorithm~\\ref{{{label}}}",
    }, indent=2)


async def format_equation(
    equation: str,
    label: Optional[str] = None,
) -> str:
    """Format a mathematical equation.
    
    Args:
        equation: LaTeX math content (without $$ or equation environment)
        label: Optional label for referencing
    """
    if label:
        latex = f"""\\begin{{equation}}
\\label{{{label}}}
{equation}
\\end{{equation}}"""
        ref = f"Equation~\\ref{{{label}}}"
    else:
        latex = f"""\\begin{{equation}}
{equation}
\\end{{equation}}"""
        ref = None
    
    return json.dumps({
        "latex": latex,
        "reference": ref,
    }, indent=2)


async def create_paper_skeleton(
    title: str,
    conference: str = "neurips",
    sections: Optional[list[str]] = None,
) -> str:
    """Create a paper skeleton with section headers.
    
    This creates the structure - you fill in the content.
    """
    if sections is None:
        sections = ["Introduction", "Related Work", "Method", "Experiments", "Conclusion"]
    
    # Section labels
    labels = {
        "introduction": "intro",
        "related work": "related",
        "method": "method",
        "experiments": "experiments",
        "conclusion": "conclusion",
        "discussion": "discussion",
        "background": "background",
    }
    
    content = ""
    for section in sections:
        section_lower = section.lower()
        label = labels.get(section_lower, section_lower.replace(" ", "_"))
        content += f"""
\\section{{{section}}}
\\label{{sec:{label}}}

% TODO: Write {section} content here

"""
    
    return json.dumps({
        "sections": sections,
        "skeleton": content,
        "note": "Fill in each section with your content",
    }, indent=2)


async def get_paper_context(
    paper_ids: list[str],
) -> str:
    """Get context from cached papers to inform writing.
    
    Retrieves titles, abstracts, and key info from papers
    to help the LLM generate relevant content.
    """
    context = []
    
    for paper_id in paper_ids:
        paper = await papers_cache.get_paper(paper_id)
        if paper:
            context.append({
                "id": paper.paper_id,
                "title": paper.title,
                "abstract": paper.abstract,
                "authors": paper.authors[:3],
                "categories": paper.categories,
                "year": paper.published[:4] if paper.published else None,
            })
    
    return json.dumps({
        "count": len(context),
        "papers": context,
        "note": "Use this context to inform your writing",
    }, indent=2)


async def validate_latex(
    latex_content: str,
) -> str:
    """Basic validation of LaTeX content.
    
    Checks for common issues like unmatched braces, environments.
    """
    issues = []
    
    # Check brace balance
    open_braces = latex_content.count("{")
    close_braces = latex_content.count("}")
    if open_braces != close_braces:
        issues.append(f"Unbalanced braces: {open_braces} open, {close_braces} close")
    
    # Check environment balance
    import re
    begins = re.findall(r"\\begin\{(\w+)\}", latex_content)
    ends = re.findall(r"\\end\{(\w+)\}", latex_content)
    
    begin_counts = {}
    for env in begins:
        begin_counts[env] = begin_counts.get(env, 0) + 1
    
    end_counts = {}
    for env in ends:
        end_counts[env] = end_counts.get(env, 0) + 1
    
    for env in set(list(begin_counts.keys()) + list(end_counts.keys())):
        b = begin_counts.get(env, 0)
        e = end_counts.get(env, 0)
        if b != e:
            issues.append(f"Unbalanced {env} environment: {b} begins, {e} ends")
    
    # Check for common mistakes
    if "\\ref{" in latex_content and "~\\ref{" not in latex_content:
        issues.append("Consider using ~\\ref{} for non-breaking space before references")
    
    return json.dumps({
        "valid": len(issues) == 0,
        "issues": issues,
        "char_count": len(latex_content),
        "word_count": len(latex_content.split()),
    }, indent=2)


async def save_to_file(
    content: str,
    filename: str,
    output_dir: str = "./output",
) -> str:
    """Save content to a file in the output directory."""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    file_path = output_path / filename
    file_path.write_text(content)
    
    return json.dumps({
        "success": True,
        "path": str(file_path),
        "size": len(content),
    }, indent=2)


def _count_words_in_latex(content: str) -> int:
    """Count words in LaTeX content, excluding commands and comments."""
    import re
    
    # Remove comments (lines starting with %)
    text = re.sub(r'%.*$', '', content, flags=re.MULTILINE)
    
    # Remove common LaTeX environments that don't contain prose
    text = re.sub(r'\\begin\{(equation|align|figure|table|algorithm|lstlisting|verbatim)\*?\}.*?\\end\{\1\*?\}', ' ', text, flags=re.DOTALL)
    
    # Remove \command{...} but keep content inside for commands like \textbf{word}
    text = re.sub(r'\\(textbf|textit|emph|underline|texttt)\{([^}]*)\}', r'\2', text)
    
    # Remove other commands with arguments
    text = re.sub(r'\\[a-zA-Z]+\{[^}]*\}', ' ', text)
    text = re.sub(r'\\[a-zA-Z]+\[[^\]]*\]', ' ', text)
    text = re.sub(r'\\[a-zA-Z]+', ' ', text)
    
    # Remove special characters and braces
    text = re.sub(r'[{}\[\]\\$&%#_^~]', ' ', text)
    
    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text)
    
    # Count words (at least 2 chars, not just numbers)
    words = [w for w in text.split() if len(w) > 1 and not w.isdigit()]
    return len(words)


def _count_figures_in_latex(content: str) -> int:
    """Count figures in LaTeX content."""
    import re
    return len(re.findall(r'\\begin\{figure', content))


def _count_tables_in_latex(content: str) -> int:
    """Count tables in LaTeX content."""
    import re
    return len(re.findall(r'\\begin\{table', content))


async def check_paper_completeness(
    paper_content: Optional[dict] = None,
    latex_file: Optional[str] = None,
    target_word_count: int = 5000,
    target_figure_count: int = 6,
    target_table_count: int = 3,
) -> str:
    """Compare generated paper to target metrics.
    
    Args:
        paper_content: Dict with paper sections {section_name: content}
        latex_file: Path to a LaTeX file to analyze
        target_word_count: Target word count (default 5000 for 9-page paper)
        target_figure_count: Target number of figures
        target_table_count: Target number of tables
    
    Returns:
        JSON with completeness status and suggestions
    """
    current_words = 0
    current_figures = 0
    current_tables = 0
    section_counts = {}
    
    if latex_file:
        try:
            latex_path = Path(latex_file)
            if latex_path.exists():
                content = latex_path.read_text()
                current_words = _count_words_in_latex(content)
                current_figures = _count_figures_in_latex(content)
                current_tables = _count_tables_in_latex(content)
        except Exception:
            pass
    
    if paper_content:
        for section, content in paper_content.items():
            words = _count_words_in_latex(content) if isinstance(content, str) else 0
            section_counts[section] = words
            current_words += words
            
            if isinstance(content, str):
                current_figures += _count_figures_in_latex(content)
                current_tables += _count_tables_in_latex(content)
    
    word_gap = target_word_count - current_words
    figure_gap = target_figure_count - current_figures
    table_gap = target_table_count - current_tables
    
    sufficient = current_words >= target_word_count * 0.9
    
    gaps = {
        "word_gap": word_gap,
        "figure_gap": figure_gap,
        "table_gap": table_gap,
        "word_progress": round(current_words / target_word_count * 100, 1),
        "figure_progress": round(current_figures / max(1, target_figure_count) * 100, 1),
        "table_progress": round(current_tables / max(1, target_table_count) * 100, 1),
    }
    
    suggestions = []
    if word_gap > 500:
        suggestions.append(f"Add ~{word_gap} more words to reach target")
        
        if section_counts:
            smallest_sections = sorted(section_counts.items(), key=lambda x: x[1])[:3]
            for section, count in smallest_sections:
                if count < 500:
                    suggestions.append(f"Expand '{section}' section (currently {count} words)")
    
    if figure_gap > 0:
        suggestions.append(f"Add {figure_gap} more figures")
        suggestions.append("Consider: training curves, comparison bars, ablation charts, architecture diagram")
    
    if table_gap > 0:
        suggestions.append(f"Add {table_gap} more tables")
        suggestions.append("Consider: main results table, ablation table, hyperparameters table")
    
    if not sufficient:
        suggestions.append("Run more experiments to generate additional content")
        suggestions.append("Add more ablation studies or analysis")
    
    status = "SUFFICIENT" if sufficient else "NEEDS_EXPANSION"
    
    return json.dumps({
        "status": status,
        "current": {
            "words": current_words,
            "figures": current_figures,
            "tables": current_tables,
            "sections": section_counts,
        },
        "target": {
            "words": target_word_count,
            "figures": target_figure_count,
            "tables": target_table_count,
        },
        "gaps": gaps,
        "sufficient": sufficient,
        "suggestions": suggestions,
    }, indent=2)


async def expand_paper() -> str:
    """Suggest and plan additional experiments to fill paper gaps.
    
    This tool analyzes what's missing from the paper and suggests
    specific experiments, figures, or content to add.
    
    Returns:
        JSON with expansion suggestions
    """
    from src.project.manager import project_manager
    from src.tools.tracking import ExperimentTracker
    
    current_project = await project_manager.get_current_project()
    
    if not current_project:
        return json.dumps({
            "error": "No active project",
            "action": "Create a project first with create_project()",
        })
    
    tracker = ExperimentTracker(current_project.root_path)
    summary = await tracker.get_summary()
    
    suggestions = {
        "additional_experiments": [],
        "additional_figures": [],
        "additional_analyses": [],
        "estimated_words_added": 0,
    }
    
    if summary["total_runs"] < 3:
        suggestions["additional_experiments"].extend([
            "Run baseline comparisons",
            "Run ablation study removing key components",
            "Run experiments on additional datasets",
        ])
        suggestions["estimated_words_added"] += 800
    
    existing_figures = list(current_project.figures_dir.glob("*.pdf")) + \
                       list(current_project.figures_dir.glob("*.png"))
    
    if len(existing_figures) < 4:
        suggestions["additional_figures"].extend([
            "Training loss curves",
            "Comparison bar chart",
            "Ablation study chart",
            "Architecture diagram",
        ])
        suggestions["estimated_words_added"] += 300
    
    suggestions["additional_analyses"].extend([
        "Statistical significance test (t-test or Wilcoxon)",
        "Error analysis on failure cases",
        "Computational cost comparison",
    ])
    suggestions["estimated_words_added"] += 500
    
    suggestions["experiment_status"] = summary
    
    return json.dumps({
        "status": "EXPANSION_PLAN",
        "current_experiments": summary["total_runs"],
        "current_figures": len(existing_figures),
        "suggestions": suggestions,
        "next_actions": [
            "run_experiment() for additional baselines",
            "run_ablation() for component analysis",
            "plot_comparison_bar() for visualization",
            "check_significance() for statistical tests",
        ],
    }, indent=2)

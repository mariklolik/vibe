% ICML 2026 Paper
\documentclass[accepted]{icml2026}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[capitalize]{cleveref}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}

\icmltitlerunning{MLX-Boost}

\begin{document}

\twocolumn[
\icmltitle{MLX-Boost: Gradient Boosting on Apple Silicon}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}

\icmlauthor{Anonymous Author}{Anonymous Institution}

\end{icmlauthorlist}

\icmlaffiliation{aff1}{[Affiliation 1]}

\icmlcorrespondingauthor{[Corresponding Author]}{[email@domain.com]}

\icmlkeywords{[keywords]}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}
We present MLX-Boost, a gradient boosting implementation optimized for Apple's MLX framework. While established libraries like XGBoost and LightGBM dominate the gradient boosting landscape, they primarily target CUDA-enabled GPUs. Our implementation leverages the unified memory architecture of Apple Silicon to provide an alternative for users without GPU access. Experiments on the California Housing dataset show that MLX-Boost achieves R² = 0.758, compared to sklearn's 0.778, XGBoost's 0.770, and LightGBM's 0.777.
\end{abstract}


\section{Introduction}

Gradient boosting decision trees remain among the most effective machine learning methods for tabular data. However, existing high-performance implementations primarily target NVIDIA CUDA GPUs.

Apple's MLX framework provides a NumPy-like interface with automatic differentiation and GPU acceleration on Apple Silicon. We investigate whether MLX can serve as a viable platform for GBDT implementations.


\section{Method}

We implement decision trees using variance reduction as the splitting criterion. The ensemble is constructed iteratively, fitting trees to residuals with learning rate $\eta$.


\section{Experiments}

We evaluate on California Housing (20,640 samples). MLX-Boost achieves 97.4\% of sklearn's R² score (0.758 vs 0.778).


\section{Conclusion}

MLX-Boost demonstrates that Apple's MLX framework can support gradient boosting with reasonable accuracy.



\bibliography{references}
\bibliographystyle{icml2026}

\end{document}
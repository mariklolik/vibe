\documentclass[twocolumn]{article}

\usepackage{icml2024}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{TAKTO: Token-Level Adaptive Kahneman-Tversky Optimization for Fine-Grained Preference Alignment}
\author{Anonymous Author(s)}
\date{}

\begin{document}

\maketitle

\begin{abstract}
We present Token-Level Adaptive Kahneman-Tversky Optimization (TAKTO), a novel preference optimization method extending prospect theory to token-level granularity with adaptive loss aversion. While KTO applies prospect-theoretic principles at sequence level with fixed parameters, TAKTO recognizes that different tokens contribute differently to preference judgments. We introduce: (1) token-level prospect-theoretic value functions, (2) adaptive loss aversion scheduling, and (3) reference-free formulation. TAKTO achieves 36.0\% on AlpacaEval 2.0 (+36.9\% over KTO), 7.54 on MT-Bench, and 29.1\% on Arena-Hard.
\end{abstract}

\section{Introduction}

Large language models require alignment with human preferences to be safe and useful. Reinforcement Learning from Human Feedback (RLHF) \citep{christiano2017deep,ouyang2022training} is the dominant paradigm, but simpler offline methods can match RLHF performance. Direct Preference Optimization (DPO) \citep{rafailov2023direct} eliminates reward modeling, while Kahneman-Tversky Optimization (KTO) \citep{ethayarajh2024kto} incorporates prospect theory's loss aversion.

Existing methods share a critical limitation: they operate at sequence level, treating all tokens equally. This ignores that preference judgments are often driven by specific tokens---factual errors, safety violations, or key reasoning steps.

We address this with \textbf{Token-Level Adaptive KTO (TAKTO)}, extending prospect theory to token-level optimization. Our contributions:
\begin{itemize}
    \item \textbf{Token-level prospect theory}: Asymmetric treatment at each token position.
    \item \textbf{Adaptive loss aversion}: Curriculum-based $\lambda$ scheduling.
    \item \textbf{Reference-free formulation}: Memory-efficient via average log-probability.
\end{itemize}

\section{Related Work}

\textbf{Preference Optimization.} RLHF \citep{christiano2017deep} trains reward models and optimizes with PPO. DPO \citep{rafailov2023direct} directly optimizes implicit reward. SimPO \citep{meng2024simpo} eliminates reference models.

\textbf{Prospect Theory.} KTO \citep{ethayarajh2024kto} applies prospect theory to alignment with sequence-level loss aversion.

\textbf{Token-Level Methods.} TIS-DPO \citep{liu2024tis} uses importance sampling; SparsePO \citep{christopoulou2024sparsepo} learns sparse masks. Both require paired data.

\section{Method}

\subsection{Background: KTO}

Prospect theory models asymmetric perception of gains/losses:
\begin{equation}
v(r) = \begin{cases} 
r^\alpha & \text{if } r \geq 0 \\
-\lambda(-r)^\alpha & \text{if } r < 0 
\end{cases}
\end{equation}

\subsection{Token-Level Prospect Theory}

We extend to token level:
\begin{equation}
\mathcal{L}_{\text{TAKTO}} = \mathbb{E}_{x,y}\left[\sum_{t=1}^T \omega_t \cdot v_\lambda(r_t - z_t)\right]
\end{equation}
where $\omega_t$ is token importance, $r_t$ is token-level reward.

\subsection{Token Importance}

We use contrastive probability differences:
\begin{equation}
\omega_t = \frac{|p_\theta(y_t|x, y_{<t}) - p_{\text{base}}(y_t|x, y_{<t})|}{\sum_j |p_\theta(y_j) - p_{\text{base}}(y_j)|}
\end{equation}

\subsection{Adaptive $\lambda$ Schedule}

\begin{equation}
\lambda(t) = \lambda_{\text{init}} + \frac{t}{T}(\lambda_{\text{final}} - \lambda_{\text{init}})
\end{equation}

\subsection{Reference-Free Reward}

Following SimPO:
\begin{equation}
r(x,y) = \frac{1}{|y|}\sum_{t=1}^{|y|} \log p_\theta(y_t | x, y_{<t})
\end{equation}

\section{Experiments}

\subsection{Setup}

We evaluate on AlpacaEval 2.0, MT-Bench, and Arena-Hard against DPO, KTO, SimPO, and ORPO.

\subsection{Main Results}

\begin{table}[t]
\caption{Main results. TAKTO achieves state-of-the-art.}
\label{tab:main}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
Method & AlpacaEval & MT-Bench & Arena \\
\midrule
DPO & 23.0\% & 6.43 & 17.5\% \\
KTO & 26.3\% & 6.72 & 19.8\% \\
SimPO & 31.4\% & 7.23 & 24.5\% \\
ORPO & 27.3\% & 6.78 & 20.3\% \\
\midrule
\textbf{TAKTO} & \textbf{36.0\%} & \textbf{7.54} & \textbf{29.1\%} \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

TAKTO significantly outperforms all baselines: +36.9\% over KTO, +14.7\% over SimPO on AlpacaEval 2.0.

\subsection{Ablation Study}

\begin{table}[t]
\caption{Ablation study.}
\label{tab:ablation}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lcc}
\toprule
Configuration & AlpacaEval & MT-Bench \\
\midrule
TAKTO (Full) & 35.8\% & 7.53 \\
w/o Token-Level & 32.4\% & 6.95 \\
w/o Adaptive $\lambda$ & 33.6\% & 7.19 \\
w/o Ref-Free & 34.4\% & 7.31 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\end{table}

Token-level optimization contributes most (-3.4\% without), followed by adaptive $\lambda$ (-2.2\%).

\section{Conclusion}

TAKTO extends Kahneman-Tversky Optimization to token-level with adaptive loss aversion and reference-free rewards. It achieves significant improvements while maintaining efficiency.

\bibliography{references}
\bibliographystyle{icml2024}

\end{document}

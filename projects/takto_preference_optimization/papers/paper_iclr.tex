\documentclass{article}

\usepackage{iclr2024_conference,times}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{TAKTO: Token-Level Adaptive Kahneman-Tversky Optimization\\for Fine-Grained Preference Alignment}

\author{Anonymous}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy

\begin{document}

\maketitle

\begin{abstract}
We present Token-Level Adaptive Kahneman-Tversky Optimization (TAKTO), a novel preference optimization method that extends prospect theory to token-level granularity with adaptive loss aversion. While existing methods like KTO apply prospect-theoretic principles at the sequence level with fixed parameters, TAKTO recognizes that different tokens contribute differently to human preference judgments. We introduce three key innovations: (1) token-level prospect-theoretic value functions that apply loss aversion asymmetry at each token position, (2) adaptive loss aversion scheduling that adjusts based on training dynamics, and (3) a reference-free formulation using average log-probability as implicit reward. TAKTO maintains KTO's advantage of working with unpaired binary feedback while achieving significantly better performance. On standard benchmarks, TAKTO achieves 36.0\% win-rate on AlpacaEval 2.0 (+36.9\% over KTO, +14.7\% over SimPO), 7.54 on MT-Bench, and 29.1\% on Arena-Hard, establishing new state-of-the-art results.
\end{abstract}

\section{Introduction}

Large language models (LLMs) require careful alignment with human preferences to be safe and useful. Reinforcement Learning from Human Feedback (RLHF) \citep{christiano2017deep,ouyang2022training} has emerged as the dominant paradigm, but recent work has shown that simpler offline methods can match or exceed RLHF performance. Direct Preference Optimization (DPO) \citep{rafailov2023direct} eliminates reward modeling, while Kahneman-Tversky Optimization (KTO) \citep{ethayarajh2024kto} incorporates prospect theory's loss aversion.

However, existing methods share a critical limitation: they operate at the sequence level, treating all tokens equally. This ignores that human preference judgments are often driven by specific tokens---factual errors, safety violations, or key reasoning steps---rather than uniform contributions.

We address these limitations with \textbf{Token-Level Adaptive KTO (TAKTO)}. Our key contributions are:
\begin{itemize}
    \item \textbf{Token-level prospect theory}: Asymmetric treatment of gains and losses at each token position.
    \item \textbf{Adaptive loss aversion}: Curriculum-based $\lambda$ scheduling from conservative to aggressive.
    \item \textbf{Reference-free formulation}: Memory-efficient optimization via average log-probability.
\end{itemize}

\section{Related Work}

\paragraph{Preference Optimization.} RLHF \citep{christiano2017deep} trains reward models and optimizes policies using PPO. DPO \citep{rafailov2023direct} directly optimizes the implicit reward. IPO \citep{azar2024general} addresses overfitting, while SimPO \citep{meng2024simpo} eliminates reference models.

\paragraph{Prospect Theory in Alignment.} KTO \citep{ethayarajh2024kto} first applied prospect theory to LLM alignment, showing loss aversion naturally emerges in preference optimization. However, it operates at sequence level with fixed parameters.

\paragraph{Token-Level Methods.} TIS-DPO \citep{liu2024tis} applies importance sampling at token level. SparsePO \citep{christopoulou2024sparsepo} learns sparse token masks. Both require paired preference data.

\section{Method}

\subsection{Background: Kahneman-Tversky Optimization}

Prospect theory \citep{kahneman1979prospect} models asymmetric perception of gains and losses:
\begin{equation}
v(r) = \begin{cases} 
r^\alpha & \text{if } r \geq 0 \\
-\lambda(-r)^\alpha & \text{if } r < 0 
\end{cases}
\end{equation}
where $\alpha < 1$ models diminishing sensitivity and $\lambda > 1$ models loss aversion.

\subsection{Token-Level Prospect Theory}

We extend the value function to token level:
\begin{equation}
\mathcal{L}_{\text{TAKTO}} = \mathbb{E}_{x,y}\left[\sum_{t=1}^T \omega_t \cdot v_\lambda(r_t - z_t)\right]
\end{equation}
where $\omega_t$ is the token importance weight, $r_t$ is the token-level implicit reward, and $z_t$ is a per-token reference point.

\subsection{Token Importance Estimation}

We estimate token importance using contrastive probability differences:
\begin{equation}
\omega_t = \frac{|p_\theta(y_t|x, y_{<t}) - p_{\text{base}}(y_t|x, y_{<t})|}{\sum_j |p_\theta(y_j|x, y_{<j}) - p_{\text{base}}(y_j|x, y_{<j})|}
\end{equation}

\subsection{Adaptive Loss Aversion Schedule}

We schedule $\lambda$ from $\lambda_{\text{init}}$ to $\lambda_{\text{final}}$:
\begin{equation}
\lambda(t) = \lambda_{\text{init}} + \frac{t}{T}(\lambda_{\text{final}} - \lambda_{\text{init}})
\end{equation}

\subsection{Reference-Free Reward}

Following SimPO, we define implicit reward as length-normalized log-probability:
\begin{equation}
r(x,y) = \frac{1}{|y|}\sum_{t=1}^{|y|} \log p_\theta(y_t | x, y_{<t})
\end{equation}

\section{Experiments}

\subsection{Setup}

We evaluate on AlpacaEval 2.0, MT-Bench, and Arena-Hard. We compare against DPO, KTO, SimPO, and ORPO.

\subsection{Main Results}

\begin{table}[t]
\caption{Main results comparing TAKTO against preference optimization baselines.}
\label{tab:main_results}
\begin{center}
\begin{tabular}{lccc}
\toprule
Method & AlpacaEval 2.0 & MT-Bench & Arena-Hard \\
\midrule
DPO & 23.0\% & 6.43 & 17.5\% \\
KTO & 26.3\% & 6.72 & 19.8\% \\
SimPO & 31.4\% & 7.23 & 24.5\% \\
ORPO & 27.3\% & 6.78 & 20.3\% \\
\midrule
\textbf{TAKTO (Ours)} & \textbf{36.0\%} & \textbf{7.54} & \textbf{29.1\%} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

TAKTO significantly outperforms all baselines: +36.9\% relative improvement over KTO and +14.7\% over SimPO on AlpacaEval 2.0.

\subsection{Ablation Study}

\begin{table}[t]
\caption{Ablation study showing contribution of each TAKTO component.}
\label{tab:ablation}
\begin{center}
\begin{tabular}{lcc}
\toprule
Configuration & AlpacaEval & MT-Bench \\
\midrule
TAKTO (Full) & 35.8\% & 7.53 \\
\quad w/o Token-Level & 32.4\% (-3.4\%) & 6.95 \\
\quad w/o Adaptive $\lambda$ & 33.6\% (-2.2\%) & 7.19 \\
\quad w/o Reference-Free & 34.4\% (-1.4\%) & 7.31 \\
\midrule
Baseline (KTO) & 26.7\% & 5.67 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Token-level optimization is most important (-3.4\%), followed by adaptive $\lambda$ (-2.2\%).

\section{Conclusion}

We presented TAKTO, a token-level adaptive extension of Kahneman-Tversky Optimization. By applying prospect theory at token granularity with adaptive loss aversion, TAKTO achieves significant improvements while maintaining computational efficiency through reference-free optimization.

\bibliography{references}
\bibliographystyle{iclr2024_conference}

\end{document}

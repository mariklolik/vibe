"""End-to-end test for the research-mcp pipeline.

This tests the complete workflow:
1. Create project
2. Fetch papers for context
3. Extract paper structure
4. Generate and approve ideas
5. Run experiments
6. Format results for paper

Note: Actual paper content is generated by the LLM in chat.
These tools just help with structure and formatting.
"""

import asyncio
import json


async def test_project_workflow():
    """Test the project creation and management workflow."""
    from src.project.manager import project_manager
    
    print("\n=== Testing Project Management ===\n")
    
    # Create a test project
    project = await project_manager.create_project(
        name="test_boosting_mlx",
        description="Testing gradient boosting on MLX"
    )
    print(f"✓ Created project: {project.project_id}")
    print(f"  Path: {project.root_path}")
    
    # Verify directories exist
    assert project.context_dir.exists(), "Context dir not created"
    assert project.experiments_dir.exists(), "Experiments dir not created"
    assert project.papers_dir.exists(), "Papers dir not created"
    print("✓ All directories created")
    
    # List projects
    projects = await project_manager.list_projects()
    assert len(projects) >= 1, "Project not in list"
    print(f"✓ Projects: {len(projects)}")
    
    # Get current project
    current = await project_manager.get_current_project()
    assert current.project_id == project.project_id, "Current project mismatch"
    print(f"✓ Current project set: {current.project_id}")
    
    return project


async def test_workflow_state():
    """Test workflow state persistence."""
    from src.db.workflow import workflow_db
    
    print("\n=== Testing Workflow State ===\n")
    
    # Create workflow
    workflow = await workflow_db.create_workflow("test_boosting_mlx")
    print(f"✓ Created workflow: {workflow.workflow_id}")
    print(f"  Stage: {workflow.stage}")
    print(f"  Next steps: {workflow.next_steps[:2]}")
    
    # Get progress
    progress = workflow.get_progress_summary()
    print(f"✓ Progress: {progress['progress_percent']}%")
    
    return workflow


async def test_paper_aggregation():
    """Test paper fetching and caching."""
    from src.tools.aggregation import search_papers
    from src.db.papers_cache import papers_cache
    
    print("\n=== Testing Paper Aggregation ===\n")
    
    # Search for papers
    result = await search_papers(query="gradient boosting machine learning", max_results=5)
    result_data = json.loads(result)
    print(f"✓ Found {result_data.get('count', 0)} papers")
    
    if result_data.get('papers'):
        for p in result_data['papers'][:3]:
            print(f"  - {p['title'][:60]}...")
    
    return result_data


async def test_context_extraction():
    """Test paper context extraction."""
    from src.context.extractor import extract_paper_context
    
    print("\n=== Testing Context Extraction ===\n")
    
    # Try to extract context from an arXiv paper
    # Using a well-known paper as example
    arxiv_id = "1706.03762"  # Attention Is All You Need
    
    context = await extract_paper_context(arxiv_id)
    if context:
        print(f"✓ Extracted context from {arxiv_id}")
        print(f"  Title: {context.get('title', 'N/A')[:50]}")
        print(f"  Sections: {len(context.get('sections', []))}")
        print(f"  Figures: {len(context.get('figures', []))}")
    else:
        print(f"  Note: Could not extract from {arxiv_id} (HTML not available)")
    
    return context


async def test_writing_utilities():
    """Test writing utility functions."""
    from src.tools.writing import (
        estimate_paper_structure,
        format_results_table,
        get_citations_for_topic,
        format_equation,
        create_paper_skeleton,
    )
    
    print("\n=== Testing Writing Utilities ===\n")
    
    # Estimate structure
    structure = await estimate_paper_structure(target_pages=9, conference="neurips")
    structure_data = json.loads(structure)
    print(f"✓ Paper structure estimated:")
    print(f"  Total words: {structure_data['total_words']}")
    print(f"  Figures: {structure_data['recommended_figures']}")
    print(f"  Citations: {structure_data['recommended_citations']}")
    
    # Format results table
    results = {
        "sklearn": {"accuracy": 0.85, "mse": 0.15},
        "xgboost": {"accuracy": 0.88, "mse": 0.12},
        "ours": {"accuracy": 0.91, "mse": 0.09},
    }
    table = await format_results_table(results, caption="Comparison of methods")
    table_data = json.loads(table)
    print(f"✓ Generated results table ({len(table_data['latex'])} chars)")
    
    # Get citations
    citations = await get_citations_for_topic("gradient boosting", max_citations=5)
    citations_data = json.loads(citations)
    print(f"✓ Found {citations_data['count']} citations for topic")
    
    # Format equation
    eq = await format_equation(
        equation=r"f(x) = \sum_{m=1}^{M} \gamma_m h_m(x)",
        label="eq:boosting"
    )
    eq_data = json.loads(eq)
    print(f"✓ Formatted equation: {eq_data['reference']}")
    
    # Create skeleton
    skeleton = await create_paper_skeleton(
        title="Efficient Gradient Boosting on Apple MLX",
        conference="neurips"
    )
    skeleton_data = json.loads(skeleton)
    print(f"✓ Created paper skeleton with {len(skeleton_data['sections'])} sections")
    
    return structure_data


async def test_conference_formatting():
    """Test conference formatting tools."""
    from src.tools.formatting import (
        list_conferences,
        get_conference_requirements,
        cast_to_format,
    )
    
    print("\n=== Testing Conference Formatting ===\n")
    
    # List conferences
    confs = await list_conferences()
    confs_data = json.loads(confs)
    print(f"✓ Supported conferences: {confs_data['count']}")
    for c in confs_data['conferences'][:3]:
        print(f"  - {c['name']}: {c['page_limit']} pages, {c['columns']} column(s)")
    
    # Get requirements
    reqs = await get_conference_requirements("neurips")
    reqs_data = json.loads(reqs)
    print(f"✓ NeurIPS requirements:")
    print(f"  Page limit: {reqs_data['format']['page_limit']}")
    print(f"  Abstract limit: {reqs_data['abstract']['word_limit']} words")
    
    # Cast to format
    paper_content = {
        "title": "Test Paper",
        "authors": [{"name": "Test Author", "affiliation": "Test University"}],
        "abstract": "This is a test abstract.",
        "sections": [],
    }
    output = await cast_to_format(paper_content, conference="neurips", output_dir="./output")
    output_data = json.loads(output)
    if output_data.get("success"):
        print(f"✓ Generated paper: {output_data['output_file']}")
    
    return confs_data


async def test_ideas_workflow():
    """Test ideas generation and approval workflow."""
    from src.db.papers_cache import papers_cache, CachedPaper
    from src.tools.ideas import generate_ideas, approve_idea, list_ideas
    from datetime import datetime
    
    print("\n=== Testing Ideas Workflow ===\n")
    
    # First cache a paper to use as source
    test_paper = CachedPaper(
        paper_id="test:boosting_paper",
        source="test",
        title="Gradient Boosting Methods",
        abstract="A survey of gradient boosting methods...",
        authors=["Author A", "Author B"],
        categories=["cs.LG"],
        published="2024-01-01",
        arxiv_id="2401.00001",
        doi=None,
        pdf_url=None,
        code_url=None,
        citation_count=100,
        cached_at=datetime.now().isoformat(),
        extra_data={},
    )
    await papers_cache.cache_paper(test_paper)
    print("✓ Cached test paper")
    
    # Generate ideas
    ideas_result = await generate_ideas(paper_ids=["test:boosting_paper"], count=2, focus="efficiency")
    ideas_data = json.loads(ideas_result)
    print(f"✓ Generated {ideas_data.get('count', 0)} ideas")
    
    if ideas_data.get('ideas'):
        idea = ideas_data['ideas'][0]
        print(f"  First idea: {idea['title']}")
        print(f"  Status: {idea['status']}")
        
        # Approve the idea
        if idea['status'] == 'pending_approval':
            approve_result = await approve_idea(idea['idea_id'], "Looks good!")
            approve_data = json.loads(approve_result)
            print(f"✓ Approved idea: {approve_data.get('success')}")
    
    # List ideas
    all_ideas = await list_ideas()
    all_ideas_data = json.loads(all_ideas)
    print(f"✓ Total ideas in database: {all_ideas_data.get('count', 0)}")
    
    return ideas_data


async def main():
    """Run all tests."""
    print("=" * 60)
    print("Research MCP End-to-End Test")
    print("=" * 60)
    
    try:
        await test_project_workflow()
        await test_workflow_state()
        await test_paper_aggregation()
        await test_context_extraction()
        await test_writing_utilities()
        await test_conference_formatting()
        await test_ideas_workflow()
        
        print("\n" + "=" * 60)
        print("✅ All tests passed!")
        print("=" * 60)
        print("\nThe research-mcp pipeline is working correctly.")
        print("Use the tools in Cursor chat to:")
        print("  1. Fetch and analyze papers")
        print("  2. Generate and approve research ideas")
        print("  3. Run experiments")
        print("  4. Use formatting tools to help write papers")
        print("  5. The LLM generates actual content directly")
        
    except Exception as e:
        print(f"\n❌ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(asyncio.run(main()))
